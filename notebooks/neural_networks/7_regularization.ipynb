{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a4073e",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6221c",
   "metadata": {},
   "source": [
    "## L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24ae3f",
   "metadata": {},
   "source": [
    "* The first form of regularization are L1 and L2 and these are used to calculate the value `penalty` so that it can be used to calculate with the `loss` value to make the generalization error less\n",
    "* L1's regularization penalty is the sum of all the absolute values of weights and bias. It is linear penalty as it is directly proportional to the weights and bias\n",
    "* L2's regularization penalty is the sum of squared weights and bias. It is non linear and it is commonly used as it doesn't affect smaller valued parameters and larger valued parameters to grow even bigger\n",
    "* We use $\\lambda$ in the equation if it is bigger then the penalty will be bigger\n",
    "\n",
    "$\\text{Loss} = \\text{data loss} + \\text{L1 for weights} + \\text{L1 for bias} + \\text{L2 for weights} + \\text{L2 for bias}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2151eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa6ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons,\n",
    "                 weight_regularizer_l1 = 0,weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
    "        # init of weights and bias\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        # init the regularization\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        # calculating gradients for parameters\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        \n",
    "        # calculating gradients for regularization\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases \n",
    "            \n",
    "        # gradients on whole values\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def regularization_loss(self,layer):\n",
    "        regularization_loss = 0\n",
    "        # Weight Loss\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            # sum of absolute values of weights\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            # sum of squared values of weights\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "    def calculate(self,output,y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0a8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_RELU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        for index, (single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output,single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues) \n",
    "    \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate=1.,decay=0.,momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self,layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer,'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            # past - future direction\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1  \n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "    def __init__(self,learning_rate=1.,decay=0.,epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / np.sqrt(layer.bias_cache) + self.epsilon\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "class Optimizer_RMSprop:\n",
    "    def __init__(self,learning_rate=0.001,decay=0.,epsilon=1e-7,rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + ( 1 - self.rho) * layer.dbiases ** 2\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e22055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64,weight_regularizer_l2=5e-4,bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_RELU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "epoches = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "learning_rates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:   4%|▍         | 225/5001 [00:00<00:04, 1098.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.757, loss: 0.724, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.800, loss: 0.574, lr: 0.04999502549496326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  10%|█         | 504/5001 [00:00<00:03, 1296.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300, acc: 0.843, loss: 0.518, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.867, loss: 0.491, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.860, loss: 0.450, lr: 0.049987528111736124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  16%|█▌        | 778/5001 [00:00<00:03, 1336.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 600, acc: 0.870, loss: 0.438, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.880, loss: 0.418, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.843, loss: 0.437, lr: 0.04998003297682575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  21%|██        | 1045/5001 [00:00<00:03, 1261.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 900, acc: 0.887, loss: 0.395, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.883, loss: 0.386, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.890, loss: 0.378, lr: 0.049972540089220974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  29%|██▉       | 1448/5001 [00:01<00:02, 1279.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1200, acc: 0.893, loss: 0.373, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.877, loss: 0.400, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.903, loss: 0.358, lr: 0.049965049447911185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  34%|███▍      | 1711/5001 [00:01<00:02, 1294.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1500, acc: 0.907, loss: 0.352, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.907, loss: 0.347, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.907, loss: 0.340, lr: 0.04995756105188642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  40%|███▉      | 1976/5001 [00:01<00:02, 1282.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1800, acc: 0.903, loss: 0.338, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.907, loss: 0.330, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.913, loss: 0.335, lr: 0.04995007490013731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  45%|████▍     | 2235/5001 [00:01<00:02, 1267.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2100, acc: 0.890, loss: 0.359, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.903, loss: 0.324, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.920, loss: 0.316, lr: 0.0499425909916551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  53%|█████▎    | 2642/5001 [00:02<00:01, 1280.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2400, acc: 0.913, loss: 0.312, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.907, loss: 0.315, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.883, loss: 0.372, lr: 0.049935109325431604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  58%|█████▊    | 2912/5001 [00:02<00:01, 1310.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2700, acc: 0.910, loss: 0.315, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.910, loss: 0.310, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.910, loss: 0.307, lr: 0.049927629900459285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  64%|██████▎   | 3181/5001 [00:02<00:01, 1312.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3000, acc: 0.910, loss: 0.304, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.913, loss: 0.301, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.913, loss: 0.298, lr: 0.04992015271573119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  69%|██████▉   | 3456/5001 [00:02<00:01, 1315.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3300, acc: 0.917, loss: 0.295, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.913, loss: 0.292, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.917, loss: 0.289, lr: 0.049912677770240964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  77%|███████▋  | 3867/5001 [00:02<00:00, 1339.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3600, acc: 0.917, loss: 0.287, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.913, loss: 0.286, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.910, loss: 0.297, lr: 0.04990520506298287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  83%|████████▎ | 4144/5001 [00:03<00:00, 1362.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3900, acc: 0.913, loss: 0.295, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.920, loss: 0.284, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.913, loss: 0.279, lr: 0.04989773459295174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  86%|████████▌ | 4281/5001 [00:03<00:00, 1323.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4200, acc: 0.917, loss: 0.275, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.917, loss: 0.274, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.917, loss: 0.271, lr: 0.04989026635914307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  93%|█████████▎| 4668/5001 [00:03<00:00, 1146.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4500, acc: 0.920, loss: 0.269, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.917, loss: 0.270, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.923, loss: 0.265, lr: 0.049882800360552884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model: 100%|██████████| 5001/5001 [00:03<00:00, 1262.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4800, acc: 0.917, loss: 0.263, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.917, loss: 0.264, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.920, loss: 0.281, lr: 0.04987533659617785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(5001),desc=\"Training the model\"):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Loss calculation\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        epoches.append(epoch)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "        learning_rates.append(optimizer.current_learning_rate)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5103bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.817, loss: 0.532\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output) \n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output, axis=1) \n",
    "if len(y_test.shape) == 2: \n",
    "    y_test = np.argmax(y_test, axis=1) \n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e6490",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349f547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self,rate):\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1,self.rate,size=inputs.shape) / self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9f38e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2, 512,weight_regularizer_l2=5e-4,bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_RELU()\n",
    "dropout_1 = Layer_Dropout(0.1)\n",
    "dense2 = Layer_Dense(512, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a96dcb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:   0%|          | 17/5001 [00:00<01:02, 80.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.099, lr: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:   2%|▏         | 116/5001 [00:01<00:59, 82.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, acc: 0.693, loss: 0.729, lr: 0.04975371909050202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:   4%|▍         | 215/5001 [00:02<00:56, 85.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, acc: 0.753, loss: 0.638, lr: 0.049507401356502806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:   6%|▌         | 308/5001 [00:03<01:03, 74.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300, acc: 0.767, loss: 0.648, lr: 0.0492635105177595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:   8%|▊         | 411/5001 [00:05<01:02, 73.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 400, acc: 0.780, loss: 0.604, lr: 0.04902201088288642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  10%|█         | 511/5001 [00:06<00:54, 83.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500, acc: 0.790, loss: 0.594, lr: 0.048782867456949125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  12%|█▏        | 610/5001 [00:07<01:02, 69.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 600, acc: 0.830, loss: 0.544, lr: 0.04854604592455945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  14%|█▍        | 720/5001 [00:09<00:49, 85.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 700, acc: 0.833, loss: 0.577, lr: 0.048311512633460556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  16%|█▋        | 816/5001 [00:10<00:51, 80.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 800, acc: 0.780, loss: 0.628, lr: 0.04807923457858551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  18%|█▊        | 916/5001 [00:11<00:48, 83.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 900, acc: 0.787, loss: 0.565, lr: 0.04784917938657352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  20%|██        | 1016/5001 [00:12<00:47, 83.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000, acc: 0.823, loss: 0.539, lr: 0.04762131530072861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  22%|██▏       | 1108/5001 [00:14<00:46, 83.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1100, acc: 0.827, loss: 0.533, lr: 0.04739561116640599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  24%|██▍       | 1212/5001 [00:15<00:43, 86.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1200, acc: 0.843, loss: 0.502, lr: 0.04717203641681212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  26%|██▌       | 1309/5001 [00:16<00:51, 71.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1300, acc: 0.837, loss: 0.540, lr: 0.04695056105920466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  28%|██▊       | 1406/5001 [00:18<00:51, 69.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1400, acc: 0.850, loss: 0.523, lr: 0.04673115566147951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  30%|███       | 1511/5001 [00:19<00:42, 82.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1500, acc: 0.830, loss: 0.518, lr: 0.046513791339132055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  32%|███▏      | 1614/5001 [00:20<00:51, 66.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1600, acc: 0.830, loss: 0.500, lr: 0.04629843974258068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  34%|███▍      | 1710/5001 [00:22<00:44, 73.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1700, acc: 0.823, loss: 0.523, lr: 0.046085073044840774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  36%|███▌      | 1807/5001 [00:23<00:51, 61.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1800, acc: 0.837, loss: 0.488, lr: 0.04587366392953806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  38%|███▊      | 1912/5001 [00:25<00:47, 64.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1900, acc: 0.853, loss: 0.498, lr: 0.04566418557925019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  40%|████      | 2018/5001 [00:26<00:38, 76.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2000, acc: 0.780, loss: 0.579, lr: 0.045456611664166556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  42%|████▏     | 2117/5001 [00:28<00:37, 76.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2100, acc: 0.847, loss: 0.519, lr: 0.045250916331055706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  44%|████▍     | 2206/5001 [00:29<00:35, 77.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2200, acc: 0.853, loss: 0.525, lr: 0.0450470741925312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  46%|████▋     | 2313/5001 [00:31<00:43, 62.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2300, acc: 0.797, loss: 0.664, lr: 0.04484506031660612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  48%|████▊     | 2411/5001 [00:32<00:30, 85.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2400, acc: 0.830, loss: 0.569, lr: 0.04464485021652753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  50%|█████     | 2519/5001 [00:33<00:28, 88.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2500, acc: 0.817, loss: 0.521, lr: 0.044446419840881816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  52%|█████▏    | 2610/5001 [00:34<00:31, 76.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2600, acc: 0.860, loss: 0.517, lr: 0.04424974556396301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  54%|█████▍    | 2717/5001 [00:36<00:26, 85.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2700, acc: 0.843, loss: 0.540, lr: 0.04405480417639544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  56%|█████▌    | 2808/5001 [00:37<00:26, 81.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2800, acc: 0.853, loss: 0.505, lr: 0.04386157287600334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  58%|█████▊    | 2911/5001 [00:38<00:25, 83.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2900, acc: 0.867, loss: 0.511, lr: 0.04367002925891961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  60%|██████    | 3010/5001 [00:39<00:24, 81.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3000, acc: 0.833, loss: 0.535, lr: 0.043480151310926564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  62%|██████▏   | 3111/5001 [00:41<00:21, 88.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3100, acc: 0.867, loss: 0.457, lr: 0.04329191739902161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  64%|██████▍   | 3210/5001 [00:42<00:27, 64.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3200, acc: 0.807, loss: 0.599, lr: 0.043105306263201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  66%|██████▋   | 3314/5001 [00:44<00:21, 78.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3300, acc: 0.840, loss: 0.485, lr: 0.0429202970084553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  68%|██████▊   | 3412/5001 [00:45<00:22, 70.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3400, acc: 0.800, loss: 0.554, lr: 0.04273686909696996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  70%|███████   | 3512/5001 [00:46<00:18, 80.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3500, acc: 0.827, loss: 0.506, lr: 0.04255500234052514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  72%|███████▏  | 3612/5001 [00:48<00:17, 79.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3600, acc: 0.847, loss: 0.545, lr: 0.042374676893088686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  74%|███████▍  | 3712/5001 [00:49<00:15, 83.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3700, acc: 0.840, loss: 0.461, lr: 0.042195873243596776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  76%|███████▌  | 3811/5001 [00:50<00:14, 82.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3800, acc: 0.830, loss: 0.590, lr: 0.04201857220891634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  78%|███████▊  | 3915/5001 [00:52<00:13, 78.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3900, acc: 0.843, loss: 0.537, lr: 0.041842754926984395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  80%|████████  | 4010/5001 [00:53<00:16, 60.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4000, acc: 0.843, loss: 0.494, lr: 0.04166840285011875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  82%|████████▏ | 4116/5001 [00:54<00:10, 84.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4100, acc: 0.807, loss: 0.541, lr: 0.041495497738495375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  84%|████████▍ | 4215/5001 [00:55<00:09, 84.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4200, acc: 0.843, loss: 0.506, lr: 0.041324021653787346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  86%|████████▌ | 4310/5001 [00:57<00:09, 70.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4300, acc: 0.823, loss: 0.550, lr: 0.041153956952961035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  88%|████████▊ | 4413/5001 [00:58<00:07, 77.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4400, acc: 0.790, loss: 0.520, lr: 0.040985286282224684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  90%|█████████ | 4515/5001 [00:59<00:06, 75.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4500, acc: 0.830, loss: 0.564, lr: 0.04081799257112535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  92%|█████████▏| 4605/5001 [01:00<00:04, 84.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4600, acc: 0.840, loss: 0.598, lr: 0.04065205902678971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  94%|█████████▍| 4713/5001 [01:02<00:03, 80.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4700, acc: 0.840, loss: 0.482, lr: 0.04048746912830479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  96%|█████████▌| 4807/5001 [01:03<00:02, 74.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4800, acc: 0.863, loss: 0.436, lr: 0.04032420662123473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model:  98%|█████████▊| 4914/5001 [01:05<00:01, 83.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4900, acc: 0.837, loss: 0.594, lr: 0.04016225551226957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training the model: 100%|██████████| 5001/5001 [01:06<00:00, 75.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5000, acc: 0.837, loss: 0.511, lr: 0.04000160006400256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(5001),desc=\"Training the model\"):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout_1.forward(activation1.output)\n",
    "    dense2.forward(dropout_1.output)\n",
    "    \n",
    "    # Loss calculation\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        epoches.append(epoch)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "        learning_rates.append(optimizer.current_learning_rate)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout_1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout_1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c39df643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.767, loss: 0.861\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dropout_1.forward(activation1.output)\n",
    "dense2.forward(dropout_1.output) \n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output, axis=1) \n",
    "if len(y_test.shape) == 2: \n",
    "    y_test = np.argmax(y_test, axis=1) \n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
