{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss Function",
   "id": "31fbd153602cf005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The loss function, also called the cost function, quantifies the error rate of a neural network model. The goal of training a deep neural network is to minimize this loss, ideally reducing it to zero, meaning the model makes perfect predictions.\n",
    "Deep neural networks generate a confidence score for each output neuron, representing the probability that the given neuron corresponds to the correct class. The loss function evaluates these confidence scores and calculates the discrepancy between the predicted values and the actual labels.\n",
    "For example, consider a model with three output neurons, producing the confidence scores:\n",
    "$$[0.25,0.55,0.20]$$\n",
    "In this case, the model predicts that the second neuron has the highest confidence score, meaning it considers this neuron the correct classification. If this prediction is correct, the ideal scenario is that the confidence scores for the incorrect neurons (first and third) should be significantly lower than the correct one.\n",
    "The loss function helps in optimizing the model by guiding updates to the neural network's parameters. Specifically, it enables optimization techniques (such as gradient descent) to reduce the confidence scores of incorrect neurons while increasing the confidence score of the correct neuron, ensuring better classification over time.\n",
    "Training a neural network involves adjusting its parameters iteratively to minimize the loss, leading to improved accuracy and more reliable predictions."
   ],
   "id": "fc7d7d5c50e71f3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Categorical Cross-Entropy Loss",
   "id": "11f587408b225a11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Categorical cross entropy loss is most used loss function with the softmax activation in output layer. The formula is $$L = -\\sum_{j}^{N} i,j \\log(\\hat{y}_{i,j})$$\n",
    "where,\n",
    "- $L$ is sample loss value\n",
    "- $i$ is the `i`th sample\n",
    "- $j$ is the label's index\n",
    "- $y$ denotes the target values\n",
    "- $\\hat{y}$ denotes the predicted value"
   ],
   "id": "5d53e92c9bc54e60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T10:04:04.244327Z",
     "start_time": "2025-04-14T10:04:04.237926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# categorical cross entropy hard coded\n",
    "import math\n",
    "softmax_output = [0.7,0.1,0.2]\n",
    "target_output = [1,0,0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
    "         math.log(softmax_output[1]) * target_output[1] +\n",
    "         math.log(softmax_output[2]) * target_output[2])\n",
    "\n",
    "print(loss)"
   ],
   "id": "33da0d3f0c8b3f23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we calculate the loss for samples",
   "id": "b123385a078c7f21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T10:10:08.122561Z",
     "start_time": "2025-04-14T10:10:08.116286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "class_targets = [0,1,1]\n",
    "\n",
    "print(\"Taking out the predicted index from the softmax output\")\n",
    "for target_index, distribution in zip(class_targets,softmax_outputs):\n",
    "    print(distribution[target_index],end=\"\\t\")"
   ],
   "id": "4182701877639801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking out the predicted index from the softmax output\n",
      "0.7\t0.5\t0.9\t"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss class",
   "id": "b3066f9ef7d8172f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T10:22:28.223285Z",
     "start_time": "2025-04-14T10:22:28.216970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_loss = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "\n",
    "class CategoricalCrossEntropyLoss(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        n_samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        correct_confidence = 0\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(n_samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidence)\n",
    "        return negative_log_likelihood"
   ],
   "id": "94cc16d3bb370c1b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T10:24:33.761866Z",
     "start_time": "2025-04-14T10:24:33.754694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "loss_function = CategoricalCrossEntropyLoss()\n",
    "loss = loss_function.calculate(softmax_outputs,class_targets)\n",
    "print(loss)"
   ],
   "id": "daa214f185c17a81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Accuracy",
   "id": "1127790d7fa747e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T10:26:59.405494Z",
     "start_time": "2025-04-14T10:26:59.397426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "accuracy = np.mean(predictions==class_targets)\n",
    "print('accuracy', accuracy)"
   ],
   "id": "7448986d8dd0e705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6666666666666666\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
