{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663cd48f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Input:\n",
    "    def forward(self,inputs):\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        for index, (single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output,single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)\n",
    "    \n",
    "    def predictions(self,outputs):\n",
    "        return np.argmax(outputs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-self.inputs))\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * (1-self.output) * self.output\n",
    "        \n",
    "    def predictions(self,outputs):\n",
    "        return (outputs > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a690e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    def predictions(self,outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_RELU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "    \n",
    "    def predictions(self,outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def regularization_loss(self,layer):\n",
    "        regularization_loss = 0\n",
    "        # Weight Loss\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                # sum of absolute values of weights\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                # sum of squared values of weights\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "    \n",
    "    def remember_trainable_layers(self,layers):\n",
    "        self.trainable_layers = layers\n",
    "        \n",
    "    def calculate(self,output,y,*,include_regularization=False):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        return data_loss,self.regularization_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def calculate(self,predictions,y):\n",
    "        comparisions = self.compare(predictions,y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20957f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Regression:\n",
    "    def __init__(self):\n",
    "        self.predictions = None\n",
    "        \n",
    "    def init(self,y,reinit=False):\n",
    "        if self.predictions is None or reinit:\n",
    "            self.predictions = np.std(y) / 250\n",
    "    \n",
    "    def compare(self,predictions,y):\n",
    "        return np.absolute(predictions - y) < self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical:\n",
    "    def init(self,y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self,predicitons,y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y,axis=1)\n",
    "        return predicitons == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783beac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set(self,*,loss,optimizer,accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuarcy = accuracy\n",
    "        \n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "        layer_count = len(self.layers)\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        for i in range(layer_count):\n",
    "            if i==0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "            \n",
    "            if hasattr(self.layers[i],'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        \n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "    \n",
    "    def train(self,X,y,*, epochs=1,print_every=1):\n",
    "        self.accuarcy.init(y)\n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            output = self.forward(X)\n",
    "            data_loss,reg_loss = self.loss.calculate(output,y,include_regularization=True) \n",
    "            loss = data_loss + reg_loss\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuarcy.calculate(predictions,y)\n",
    "            self.backward(output,y)\n",
    "            \n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "            \n",
    "            if not epoch % print_every:\n",
    "                print(f\"Epoch : {epoch} acc: {accuracy} loss: {loss}\")\n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.input_layer.forward(X)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output)\n",
    "            \n",
    "        return layer.output\n",
    "    \n",
    "    def backward(self,output,y):\n",
    "        self.loss.backward(output,y)\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
