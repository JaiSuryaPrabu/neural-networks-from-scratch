{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d88ba431f58ad1",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f32337161492e6",
   "metadata": {},
   "source": [
    "In this notebook we are implementing the `dense` layer code"
   ]
  },
  {
   "cell_type": "code",
   "id": "73f2b420dcc63bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T09:40:47.334693Z",
     "start_time": "2025-04-14T09:40:47.064239Z"
    }
   },
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "53a59eb6",
   "metadata": {},
   "source": [
    "# DenseLayer\n",
    "The `DenseLayer` class represents a fully connected layer in a deep neural network. It initializes weights and biases and performs a forward pass using matrix multiplication.\n",
    "\n",
    "## Constructor \n",
    "### Parameters\n",
    "* `n_inputs` (int) - number of inputs\n",
    "* `n_neurons` (int) - number of neurons in the dense layer\n",
    "### Attributes\n",
    "* `self.weights` (numpy.ndarray) - weights are initialized by setting little random variation of normal distribution\n",
    "* `self.biases` (numpy.ndarray) - bias matrix is initialized by elements of zeros with the shape of `(1,number of neurons)`\n",
    "## Methods\n",
    "### Forward\n",
    "#### Parameters\n",
    "* `inputs` (numpy.ndarray) - previous layer output is provided as input\n",
    "#### Computation\n",
    "$Y=XW+C$\n",
    "\n",
    "* Y - output\n",
    "* W - weights\n",
    "* X - inputs\n",
    "* C - Bias"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5dfa8c35b5dffee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T09:40:55.025189Z",
     "start_time": "2025-04-14T09:40:55.017212Z"
    }
   },
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.outputs = np.dot(inputs,self.weights) + self.biases"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "10498c1ea8c90a70",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ActivationRELU\n",
    "`ActivationRELU` class defines the Rectified Linear Unit which activates the neuron if it has value greater than 0 else it doesn't activates the neuron."
   ],
   "id": "bda63840343ba4df"
  },
  {
   "cell_type": "code",
   "id": "5229ecda5e1540ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T09:40:57.126972Z",
     "start_time": "2025-04-14T09:40:57.120817Z"
    }
   },
   "source": [
    "# relu code\n",
    "class ActivationRELU:\n",
    "    def forward(self,inputs):\n",
    "        self.outputs = np.maximum(0,inputs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ActivationSoftMax\n",
    "`ActivationSoftMax` class defines the softmax algorithm where the formula is:\n",
    "\n",
    "$$S(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{n} e^{x_j - \\max(x)}}$$\n",
    "\n",
    "\n",
    "\n",
    "where:\n",
    "- $x_i$ represents the input value (logit) for class $i$,\n",
    "- $\\max(x)$ is the maximum value in the input vector (used for numerical stability),\n",
    "- $e^{x_i - \\max(x)}$ is the exponentiation of the stabilized input,\n",
    "- The denominator $\\sum_{j=1}^{n} e^{x_j - \\max(x)}$ ensures the output values sum to 1,\n",
    "- The final output $S(x_i)$ represents the probability distribution over $n$ possible classes."
   ],
   "id": "f1cc7e930830aa21"
  },
  {
   "cell_type": "code",
   "id": "6c00c7fa96f946ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T09:40:59.659610Z",
     "start_time": "2025-04-14T09:40:59.654374Z"
    }
   },
   "source": [
    "class ActivationSoftMax:\n",
    "    def forward(self,inputs):\n",
    "        exp_val = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        prob = exp_val / np.sum(exp_val,axis=1,keepdims=True)\n",
    "        self.outputs = prob"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building the layer",
   "id": "e4c2261f9a92e84f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T09:41:04.361554Z",
     "start_time": "2025-04-14T09:41:04.267374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.array([[0.5, -0.2, 0.1, 0.7]])\n",
    "\n",
    "dense1 = DenseLayer(4,3)\n",
    "dense1.forward(X)\n",
    "activation1 = ActivationRELU()\n",
    "activation1.forward(dense1.outputs)\n",
    "\n",
    "dense2 = DenseLayer(3,3)\n",
    "dense2.forward(activation1.outputs)\n",
    "activation2 = ActivationRELU()\n",
    "activation2.forward(dense2.outputs)\n",
    "\n",
    "dense_output = DenseLayer(3,2)\n",
    "dense_output.forward(activation2.outputs)\n",
    "activation_output = ActivationSoftMax()\n",
    "activation_output.forward(dense_output.outputs)\n",
    "\n",
    "print(activation_output.outputs)"
   ],
   "id": "bfc2f6bff94f40c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50000003 0.49999997]]\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
