{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61732ea9",
   "metadata": {},
   "source": [
    "# Backpropagation for single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10925d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2508b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample inputs, weights and biases\n",
    "sample_inputs = np.array([[1.0, 2.0, 3.0, 2.5],[2.0, 5.0, -1.0, 2.0],[-1.5, 2.7, 3.3, -0.8]])\n",
    "sample_weights = np.array([[0.2, 0.8, -0.5, 1.0],[0.5, -0.91, 0.26, -0.5],[-0.26, -0.27, 0.17, 0.87]]).T\n",
    "sample_biases = np.array([[2.0, 3.0, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c24236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "layer_outputs = np.dot(sample_inputs, sample_weights) + sample_biases\n",
    "relu_outputs = np.maximum(0,layer_outputs)\n",
    "\n",
    "# backward pass\n",
    "dvalues = np.array([[1.0, 1.0, 1.0],[2.0, 2.0, 2.0],[3.0, 3.0, 3.0]]) # sample next layer gradients\n",
    "drelu = dvalues.copy()\n",
    "drelu[layer_outputs <= 0] = 0 # gradients for relu\n",
    "dinputs = np.dot(drelu,sample_weights.T) # gradients for inputs\n",
    "dweights = np.dot(sample_inputs.T,drelu) # gradients for weights\n",
    "dbiases = np.sum(drelu,axis=0,keepdims=True) # gradients for biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b97e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients with respect to Inputs :  [[ 0.44 -0.38 -0.07  1.37]\n",
      " [-0.12  1.06 -0.66  3.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n",
      "Gradients with respect to weights :  [[ 0.5 -3.5  0.5]\n",
      " [20.1 10.1 20.1]\n",
      " [10.9 12.9 10.9]\n",
      " [ 4.1  0.1  4.1]]\n",
      "Gradients with respect to biases :  [[6. 4. 6.]]\n",
      "Gradients with respect to relu :  [[1. 1. 1.]\n",
      " [2. 0. 2.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradients with respect to Inputs : \",dinputs)\n",
    "print(\"Gradients with respect to weights : \",dweights)\n",
    "print(\"Gradients with respect to biases : \",dbiases)\n",
    "print(\"Gradients with respect to relu : \",drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d52cd6",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd421e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDense:\n",
    "    \"\"\" Fully connected Layer \"\"\"\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        \"\"\" y = mx + c \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        \n",
    "class ActivationRELU:\n",
    "    \"\"\" Rectified Linear Unit Activation Layer \"\"\"\n",
    "    def forward(self,inputs):\n",
    "        \"\"\" y = max(0,x) \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0,inputs)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class ActivationSoftmax:\n",
    "    \"\"\" Softmax Activation Layer - moslty used for classification \"\"\"\n",
    "    def forward(self,inputs):\n",
    "        \"\"\" y = exp(x) / sum(exp(x)) \"\"\"\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        prob_values = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        \n",
    "        self.outputs = prob_values\n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_loss = self.forward(output,y)\n",
    "        mean_loss = np.mean(sample_loss)\n",
    "        return mean_loss\n",
    "    \n",
    "class LossCategoricalCrossEntropy(Loss):\n",
    "    \"\"\" Categorical Cross Entropy Loss \"\"\"\n",
    "    def forward(self,y_pred,y_true):\n",
    "        \"\"\" y = -sum(y_true * log(y_pred)) \"\"\"\n",
    "        n_samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        correct_confidence = 0\n",
    "        \n",
    "        # taking out the correct true value from y_pred \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(n_samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # this is for one-hot encoded labels\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidence)\n",
    "        return negative_log_likelihood\n",
    "       \n",
    "# softmax and cross-entropy loss are combined for optimzation\n",
    "class ActivationSoftmaxLossCategoricalCrossentropy:\n",
    "    def __init__(self):\n",
    "        self.activation = ActivationSoftmax()\n",
    "        self.loss = LossCategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.outputs\n",
    "        return self.loss.calculate(self.outputs,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        self.dinputs /= samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6eafd",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d919d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X,y = spiral_data(samples=100,classes=3)\n",
    "\n",
    "dense1= LayerDense(2,3)\n",
    "activation1 = ActivationRELU()\n",
    "dense2 = LayerDense(3,3)\n",
    "loss_activation = ActivationSoftmaxLossCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67347b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1.0986104\n",
      "accuracy :  0.34\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.outputs)\n",
    "dense2.forward(activation1.outputs)\n",
    "\n",
    "loss = loss_activation.forward(dense2.outputs,y)\n",
    "print(\"loss : \",loss)\n",
    "\n",
    "predictions = np.argmax(loss_activation.outputs,axis=1)\n",
    "if len(y.shape) == 2: \n",
    "   y = np.argmax(y,axis=1) \n",
    "accuracy = np.mean(predictions==y)\n",
    "print(\"accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e031f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "loss_activation.backward(loss_activation.outputs,y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8608e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(dense1.dweights) \n",
    "print(dense1.dbiases) \n",
    "print(dense2.dweights) \n",
    "print(dense2.dbiases) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
